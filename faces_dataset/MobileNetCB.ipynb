{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils_path = \"../utils\"\n",
    "models_path = \"../models\"\n",
    "data_path = \"../data/celebs_20\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(1, utils_path)\n",
    "sys.path.insert(2, models_path)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import neptune.new as neptune\n",
    "from neptune.new.types import File\n",
    "\n",
    "from config import NEPTUNE_TOKEN\n",
    "\n",
    "import time \n",
    "import copy\n",
    "import random\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms as transformss\n",
    "\n",
    "from MBNV3 import MobileNetV3\n",
    "from MBNV3CBAM import MobileNetV3CBAM\n",
    "\n",
    "from dataloader import mixedSets\n",
    "from functions import train, evaluate, confusion, unzip_files, unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialising GereralTorch class\n",
    "\n",
    "#Report multiple hyperparameters using a dictionary\n",
    "hyper_params = {\n",
    "    'learning_rate': 0.001,\n",
    "    'epochs': 1500,\n",
    "    'batch_size': 16,\n",
    "    'image_size': 112,\n",
    "    'image_channels': 3,\n",
    "    'output_size': len(os.listdir(data_path)) - 1,\n",
    "    'num_layers': 'na',\n",
    "    'train_val_test_split': [0.8, 0.1, 0.1],\n",
    "    'device': 'mps',\n",
    "    'model_name': 'MobileNet3 with CBAM inserted instead of SE starting from a fresh model with no preloaded weights.',\n",
    "    'criterion': 'CrossEntropyLoss',\n",
    "    'optimizer': 'Adam',\n",
    "    'dataset': 'Celebrities 20',\n",
    "    'best_model_path': 'output/MN3LCB_celebs20.pt',\n",
    "    'loaded_model_path': 'output/MN3LCB_celebs20.pt',\n",
    "    'save_at_end': True,\n",
    "}\n",
    "\n",
    "print(f'Output Size: {hyper_params[\"output_size\"]}')\n",
    "\n",
    "#Setting the device\n",
    "device = torch.device(hyper_params['device'])\n",
    "\n",
    "#checking if the model is to be a loaded one and if so loading it\n",
    "if hyper_params['loaded_model_path']:\n",
    "    state_dict = torch.load(hyper_params['loaded_model_path']) #Loading the state dict\n",
    "    model = MobileNetV3CBAM(mode='large') #Loading the model as a backbone\n",
    "    model.load_state_dict(state_dict, strict= True) #Loading the state dict into the model\n",
    "    print('Model Loaded')\n",
    "else:\n",
    "    # Loading a fresh model\n",
    "    model = MobileNetV3CBAM(mode='large')\n",
    "\n",
    "#passing the model to the device\n",
    "model.to(device)\n",
    "\n",
    "# Setting the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss().to(device) #Setting the loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hyper_params['learning_rate']) #Setting the optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the experiment with the API key stored in config.py\n",
    "run = neptune.init(project=\"leothesouthafrican/Thesis\", api_token=NEPTUNE_TOKEN,\n",
    "                                    source_files=[\"*.ipynb\"])\n",
    "\n",
    "#Initialising the model\n",
    "run[\"model\"] = model\n",
    "neptune_model = neptune.init_model_version(\n",
    "    with_id=\"THES-MBNV3CBAM-10\",\n",
    "    project=\"leothesouthafrican/Thesis\",\n",
    "    api_token=NEPTUNE_TOKEN,\n",
    ")\n",
    "\n",
    "#Logging the hyperparameters to the run on neptune\n",
    "run[\"hyper-parameters\"] = hyper_params\n",
    "run[\"train/images\"].track_files(data_path)\n",
    "\n",
    "#Logging the hyperparameters to the run on neptune\n",
    "run[\"hyper-parameters\"] = hyper_params\n",
    "run[\"train/images\"].track_files(data_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining transforms\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "                transforms.Resize((hyper_params['image_size'],hyper_params['image_size'])),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]),\n",
    "                transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "                transforms.RandomCrop(hyper_params['image_size'], padding=2)], p=0.4),\n",
    "                transforms.RandomAffine(degrees=10, translate=(0.1,0.1), scale=(0.8,1.2)),\n",
    "                transforms.RandomRotation(35),\n",
    "                transforms.RandomHorizontalFlip(0.2),\n",
    "                transforms.RandomVerticalFlip(0.15),\n",
    "                transforms.RandomErasing(0.2),\n",
    "            ])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "                transforms.Resize((hyper_params['image_size'],hyper_params['image_size'])),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "\n",
    "#Logging the transforms to neptune\n",
    "run[\"train/transforms\"] = train_transform\n",
    "run[\"test/transforms\"] = test_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataset\n",
    "train_dataset = mixedSets(data_path, train_transform, hyper_params['train_val_test_split']).get_train_dataset()\n",
    "val_dataset = mixedSets(data_path, test_transform, hyper_params['train_val_test_split']).get_val_dataset()\n",
    "test_dataset = mixedSets(data_path, test_transform, hyper_params['train_val_test_split']).get_test_dataset()\n",
    "\n",
    "# Creating the dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                            batch_size=hyper_params['batch_size'],\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=1)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                            batch_size=hyper_params['batch_size'],\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=1)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                            batch_size=hyper_params['batch_size'],\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=1)\n",
    "\n",
    "#Logging the dataset to neptune\n",
    "run[\"train/dataset\"] = train_dataset\n",
    "run[\"val/dataset\"] = val_dataset\n",
    "run[\"test/dataset\"] = test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display random images from the dataset\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    #drop axis labels\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "#save image to output folder\n",
    "torchvision.utils.save_image(torchvision.utils.make_grid(images), 'output/train_images.png')\n",
    "\n",
    "#Logging the images to neptune\n",
    "#run[\"train/images\"].upload('output/train_images.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, criterion, optimizer, hyper_params, train_loader, val_loader, run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(hyper_params['best_model_path'])) #Loading the state dict into the model\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion, device, run) \n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neptune_model[\"model\"].upload(hyper_params['best_model_path'])\n",
    "neptune_model[\"validation/acc\"] = test_acc\n",
    "neptune_model[\"validation/loss\"] = test_loss\n",
    "neptune_model[\"validation/dataset\"] = test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion(model, test_loader, run, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16 (main, Dec  7 2022, 10:06:04) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3eacf7c38dfd06f03ba84c25998c3b980b49954d2ca7328ee71b9a00e83ad9fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
