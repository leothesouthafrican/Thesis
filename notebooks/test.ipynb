{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/Users/leo/Desktop/subjects_0/\"\n",
    "\n",
    "clone_path = \"/Users/leo/Desktop/Thesis/data/subjects_copy/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "def copy_folder(src_path, dst_path, num_folders_keep):\n",
    "    try:\n",
    "        shutil.rmtree(dst_path)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting {dst_path}: {e}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(dst_path, exist_ok=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating {dst_path}: {e}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        all_folders = os.listdir(src_path)\n",
    "        num_folders = len(all_folders)\n",
    "        keep_folders = set(random.sample(all_folders, min(num_folders_keep, num_folders)))\n",
    "        \n",
    "        for folder in keep_folders:\n",
    "            src_folder = os.path.join(src_path, folder)\n",
    "            dst_folder = os.path.join(dst_path, folder)\n",
    "            shutil.copytree(src_folder, dst_folder)\n",
    "            print(f\"Copied {src_folder} to {dst_folder}\")\n",
    "            \n",
    "        print(f\"Successfully copied {num_folders_keep} out of {num_folders} folders from {src_path} to {dst_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error copying folders from {src_path} to {dst_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "def split_dataset(src_folder, dst_folder, train_ratio=0.7, val_ratio=0.2, test_ratio=0.1):\n",
    "    # Create destination directories\n",
    "    train_dir = os.path.join(dst_folder, 'train')\n",
    "    val_dir = os.path.join(dst_folder, 'val')\n",
    "    test_dir = os.path.join(dst_folder, 'test')\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(val_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "    # Iterate over each subdirectory in the source folder\n",
    "    for subdir_name in os.listdir(src_folder):\n",
    "        subdir = os.path.join(src_folder, subdir_name)\n",
    "        if not os.path.isdir(subdir):\n",
    "            continue\n",
    "        \n",
    "        # Create subdirectories in the destination folders\n",
    "        train_subdir = os.path.join(train_dir, subdir_name)\n",
    "        val_subdir = os.path.join(val_dir, subdir_name)\n",
    "        test_subdir = os.path.join(test_dir, subdir_name)\n",
    "        os.makedirs(train_subdir, exist_ok=True)\n",
    "        os.makedirs(val_subdir, exist_ok=True)\n",
    "        os.makedirs(test_subdir, exist_ok=True)\n",
    "\n",
    "        # Get a list of all files in the subdirectory\n",
    "        files = os.listdir(subdir)\n",
    "        num_files = len(files)\n",
    "\n",
    "        # Shuffle the list of files randomly\n",
    "        random.shuffle(files)\n",
    "\n",
    "        # Calculate the number of files for each split\n",
    "        num_train_files = int(num_files * train_ratio)\n",
    "        num_val_files = int(num_files * val_ratio)\n",
    "        num_test_files = int(num_files * test_ratio)\n",
    "\n",
    "        # Split the files into train, validation, and test sets\n",
    "        train_files = files[:num_train_files]\n",
    "        val_files = files[num_train_files:num_train_files + num_val_files]\n",
    "        test_files = files[num_train_files + num_val_files:num_train_files + num_val_files + num_test_files]\n",
    "\n",
    "        # Copy the files to the appropriate directories\n",
    "        for file in train_files:\n",
    "            src_path = os.path.join(subdir, file)\n",
    "            dst_path = os.path.join(train_subdir, file)\n",
    "            shutil.copy(src_path, dst_path)\n",
    "\n",
    "        for file in val_files:\n",
    "            src_path = os.path.join(subdir, file)\n",
    "            dst_path = os.path.join(val_subdir, file)\n",
    "            shutil.copy(src_path, dst_path)\n",
    "\n",
    "        for file in test_files:\n",
    "            src_path = os.path.join(subdir, file)\n",
    "            dst_path = os.path.join(test_subdir, file)\n",
    "            shutil.copy(src_path, dst_path)\n",
    "\n",
    "    print('Done splitting dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10d683690>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/Users/leo/Desktop/Thesis/utils/\")\n",
    "sys.path.append(\"/Users/leo/Desktop/Thesis/models/\")\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.models import mobilenet_v3_large, mobilenet_v3_small\n",
    "from torchvision.models import MobileNet_V3_Large_Weights as weights_large, MobileNet_V3_Small_Weights as weights_small\n",
    "import torchvision.transforms as transforms\n",
    "from torchinfo import summary\n",
    "from helper_functions import delete_ds_store, plot_metrics, test, train, set_device, mean_std_finder, load_model\n",
    "from CBAM import _CBAM\n",
    "from MBNV3_2 import MBNV3Creator\n",
    "from config import comet_token, project_name, workspace\n",
    "from comet_ml import Experiment\n",
    "\n",
    "#setting the seed for reproducibility\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "prms = {\n",
    "    \"backbone\": mobilenet_v3_small,\n",
    "    \"weights\": weights_small.IMAGENET1K_V1,\n",
    "    \"module\": _CBAM,\n",
    "    \"module_alias\": \"CBAM\",\n",
    "    \"data_path\": \"../data/faces_50/\",\n",
    "    \"batch_size\": 16,\n",
    "    \"img_size\": 224,\n",
    "    \"num_classes\": 50,\n",
    "    \"epochs\": 5,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"device\": set_device(),\n",
    "    \"model\": None,\n",
    "    \"experiment\": None,\n",
    "    \"load_model\": False,\n",
    "}\n",
    "prms[\"model_save_path\"] = f\"./output/{prms['module_alias']}_{prms['data_path'].split('/')[-2]}.pth\"\n",
    "\n",
    "model = prms[\"model\"] = MBNV3Creator(prms[\"backbone\"], prms[\"num_classes\"], prms[\"weights\"], prms[\"device\"], prms[\"module\"])\n",
    "delete_ds_store(prms[\"data_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean, std = mean_std_finder(prms[\"data_path\"])\n",
    "mean, std = torch.tensor([0.3860, 0.3119, 0.2690]), torch.tensor([0.2279, 0.2167, 0.2135])\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((prms[\"img_size\"], prms[\"img_size\"])),\n",
    "    transforms.RandAugment(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((prms[\"img_size\"], prms[\"img_size\"])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accessing the data\n",
    "train_data = torchvision.datasets.ImageFolder(prms[\"data_path\"] + \"train/\", transform=train_transform)\n",
    "val_data = torchvision.datasets.ImageFolder(prms[\"data_path\"] + \"val/\", transform=test_transform)\n",
    "test_data = torchvision.datasets.ImageFolder(prms[\"data_path\"] + \"test/\", transform=test_transform)\n",
    "\n",
    "#Creating the data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=32, shuffle=False, num_workers=0)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "\n",
    "class res18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(res18, self).__init__()\n",
    "        self.resnet = resnet18(pretrained=True)\n",
    "        self.model_children = list(self.resnet.children())\n",
    "        self.feature_conv = nn.Sequential(*self.model_children[:-2])\n",
    "        self.classifier = nn.Sequential(*self.model_children[-2:])\n",
    "\n",
    "        self.gradients = None\n",
    "\n",
    "        del self.resnet\n",
    "\n",
    "    def activations_hook(self, grad):\n",
    "        self.gradients = grad\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_conv(x)\n",
    "        print(x.shape)\n",
    "        x.register_hook(self.activations_hook)\n",
    "        print(x.shape)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    def get_activations_gradient(self):\n",
    "        return self.gradients\n",
    "    \n",
    "    def get_activations(self, x):\n",
    "        return self.feature_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 7, 7])\n",
      "torch.Size([1, 512, 7, 7])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (512x1 and 512x1000)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m img, _ \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(test_loader))\n\u001b[1;32m      7\u001b[0m img \u001b[39m=\u001b[39m img[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m pred \u001b[39m=\u001b[39m model(img)\n",
      "File \u001b[0;32m~/Desktop/Thesis/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[69], line 23\u001b[0m, in \u001b[0;36mres18.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m x\u001b[39m.\u001b[39mregister_hook(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivations_hook)\n\u001b[1;32m     22\u001b[0m \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> 23\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclassifier(x)\n\u001b[1;32m     24\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Desktop/Thesis/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/Thesis/env/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/Thesis/env/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/Thesis/env/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (512x1 and 512x1000)"
     ]
    }
   ],
   "source": [
    "model = res18()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "img, _ = next(iter(test_loader))\n",
    "\n",
    "img = img[0].unsqueeze(0)\n",
    "\n",
    "\n",
    "pred = model(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
